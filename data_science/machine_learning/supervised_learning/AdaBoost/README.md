# AdaBoost
AdaBoost (Adaptive Boosting) is a boosting algorithm that can be used to train weak classifiers to perform better by combining their predictions. Boosting algorithms work by building a model from a series of weaker models, each of which is able to perform better than random guessing. AdaBoost is an adaptive algorithm because it adjusts the weight of each weak classifier based on its performance, so that the next weak classifier focuses more on the examples that were misclassified by the previous ones.

The general idea of boosting is to train a sequence of weak classifiers, each of which is able to make predictions that are slightly better than random guessing. The predictions of the weak classifiers are combined using a weighted majority vote, where the weights are chosen to emphasize the more accurate classifiers.

AdaBoost is a popular boosting algorithm because it is simple to implement and can achieve good performance on a variety of tasks. It has been applied to a wide range of problems, including image classification, text classification, and object detection.

To further understand:
* [Video: AdaBoost, clearly explained !](https://www.youtube.com/watch?v=LsK-xG1cLYA)